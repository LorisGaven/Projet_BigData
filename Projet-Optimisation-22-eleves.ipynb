{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Collabrative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Preprocessing of data\n",
    "\n",
    "1. Data format\n",
    "\n",
    "We are going to preprocess a rating file in the following csv format:  \n",
    "```\n",
    "UserID::MovieID::Rating::Timestamp\n",
    "```\n",
    "\n",
    "2. Prepare data for cross-validation\n",
    "\n",
    "Splitting Data for a user-based N-fold cross-validation. Store each partition into a new csv file. \n",
    "\n",
    "It is convinient to use a python package lenskit. It can be installed by following https://lkpy.readthedocs.io/en/stable/install.html\n",
    "\n",
    "Use the function lenskit.crossfold.partition_rows to partition all the ratings into N train-test partitions.\n",
    "\n",
    "3. Convert to a list format\n",
    "\n",
    "Convert data into a list format for fast processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lenskit\n",
      "  Downloading lenskit-0.14.2-py3-none-any.whl (74 kB)\n",
      "Collecting seedbank>=0.1.0\n",
      "  Downloading seedbank-0.1.2-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: numba<0.57,>=0.51 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from lenskit) (0.55.1)\n",
      "Collecting csr>=0.3.1\n",
      "  Downloading csr-0.4.3-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: scipy>=1.2 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from lenskit) (1.7.3)\n",
      "Requirement already satisfied: cffi>=1.12.2 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from lenskit) (1.15.0)\n",
      "Requirement already satisfied: psutil>=5 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from lenskit) (5.8.0)\n",
      "Collecting binpickle>=0.3.2\n",
      "  Downloading binpickle-0.3.4-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pandas==1.*,>=1.0 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from lenskit) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from lenskit) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from pandas==1.*,>=1.0->lenskit) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from pandas==1.*,>=1.0->lenskit) (2021.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from binpickle>=0.3.2->lenskit) (1.0.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from cffi>=1.12.2->lenskit) (2.21)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from numba<0.57,>=0.51->lenskit) (0.38.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from numba<0.57,>=0.51->lenskit) (61.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas==1.*,>=1.0->lenskit) (1.16.0)\n",
      "Collecting anyconfig\n",
      "  Downloading anyconfig-0.13.0-py2.py3-none-any.whl (87 kB)\n",
      "Installing collected packages: anyconfig, seedbank, csr, binpickle, lenskit\n",
      "Successfully installed anyconfig-0.13.0 binpickle-0.3.4 csr-0.4.3 lenskit-0.14.2 seedbank-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install lenskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 610 users, 9724 movies and the rating matrix has 1.699968 percent of non-zero value.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read a DataFrame of ratings from the csv ratings file\n",
    "csvroot = 'data'\n",
    "ratings = pd.read_csv(csvroot + '/ratings.csv')\n",
    "#ratings = ratings.iloc[:1000]\n",
    "ratings = ratings.rename(columns={'userId': 'user', 'movieId': 'item'})\n",
    "\n",
    "def unique(list1):\n",
    "    # insert the list to the set\n",
    "    list_set = set(list1)\n",
    "    # convert the set to the list\n",
    "    unique_list = (list(list_set))\n",
    "    # use a dictionary to store the index of eac element in the list\n",
    "    unique_dict = {}\n",
    "    for i in range(len(unique_list)):\n",
    "        unique_dict[unique_list[i]] = i \n",
    "    return unique_list, unique_dict\n",
    "\n",
    "lst_users, dic_users = unique(ratings['user'])\n",
    "lst_items, dic_items = unique(ratings['item'])\n",
    "\n",
    "M = len(lst_users)\n",
    "N = len(lst_items)\n",
    "matrixSparsity = len(ratings) / (M*N)\n",
    "print(\"We have %d users, %d movies and the rating matrix has %f percent of non-zero value.\\n\" % (M, N, 100*matrixSparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100836"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train0 : 80836 lines\n",
      "test0 : 20000 lines\n",
      "train1 : 80836 lines\n",
      "test1 : 20000 lines\n",
      "train2 : 80836 lines\n",
      "test2 : 20000 lines\n",
      "train3 : 80836 lines\n",
      "test3 : 20000 lines\n",
      "train4 : 80836 lines\n",
      "test4 : 20000 lines\n"
     ]
    }
   ],
   "source": [
    "# 2 fold cross-validation, store each partition (both train and test) in a seperate csv file.\n",
    "# Point 1: report the number of (user,iter,rating) items in each file.\n",
    "\n",
    "import lenskit.crossfold as xf\n",
    "\n",
    "nbSample = 0\n",
    "for train, test in xf.sample_rows(ratings, 5, 20000):\n",
    "    train.to_csv(csvroot + '/train-%d.csv' % (nbSample,))\n",
    "    print(f'train{nbSample} : {len(train)} lines')\n",
    "    test.to_csv(csvroot + '/test-%d.csv' % (nbSample,))\n",
    "    print(f'test{nbSample} : {len(test)} lines')\n",
    "    nbSample += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_DF_to_RDD(DF):\n",
    "    \"\"\" \n",
    "    This function converts a rating dataframe into a dictionary of lists\n",
    "    Args:\n",
    "        DF: a dataframe which contains 'user', 'item' and 'rating' columns\n",
    "    Returns:\n",
    "        RDD: a dictionary which contains \n",
    "            'total': the total number of rating\n",
    "            'users': a list of user id for each rating\n",
    "            'items': a list of item id for each rating\n",
    "            'ratings': a list of ratings\n",
    "    \"\"\" \n",
    "    RDD = {'total':0,'users':[],'items':[],'ratings':[]} \n",
    "    \n",
    "    for i in range(len(DF)):\n",
    "        RDD['total'] += 1\n",
    "        RDD['users'].append(DF.user[i])\n",
    "        RDD['items'].append(DF.item[i])\n",
    "        RDD['ratings'].append(DF.rating[i])\n",
    "    \n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv files from a partition of the cross-validation\n",
    "# convert them to RDD using convert_DF_to_RDD\n",
    "\n",
    "train = convert_DF_to_RDD(pd.read_csv(csvroot + '/train-0.csv'))\n",
    "test = convert_DF_to_RDD(pd.read_csv(csvroot + '/test-0.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Gradient-descent algorithm\n",
    "\n",
    "Based on the preprocessing, we are going to develop a method to find optimal P and Q on training data. It contains: \n",
    "\n",
    "1. compute the objective and the gradient of the objective function\n",
    "2. implement the gradient-descent algroithm\n",
    "3. measure the speed of this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume now you have obtained trainRDD and testRDD\n",
    "# Compute the objective funtion\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def computeMSE(RDD,P,Q,la=0):\n",
    "    \"\"\" \n",
    "    This function computes regularized Mean Squared Error (MSE)\n",
    "    Args:\n",
    "        RDD: a dict of list of userID, itemID, Rating\n",
    "        P: user's features matrix (M by K)\n",
    "        Q: item's features matrix (N by K)\n",
    "        la: lambda parameter for regulization (see definition in the project description)\n",
    "    Returns:\n",
    "        mse: mean squared error\n",
    "    \"\"\" \n",
    "    \n",
    "    se = 0\n",
    "    ratings = RDD['ratings']\n",
    "    users = RDD['users']\n",
    "    items = RDD['items']\n",
    "    total = RDD['total']\n",
    "    for n in range(total):\n",
    "        r = ratings[n]\n",
    "        i = dic_users[users[n]]\n",
    "        j = dic_items[items[n]]\n",
    "        hQ = np.copy(Q[j,:])\n",
    "        hQ[hQ > 0] = 0\n",
    "        hP = np.copy(P[i,:])\n",
    "        hP[hP > 0] = 0\n",
    "        se += (r - Q[j,:].T @ P[i,:])**2 + la * (np.linalg.norm(hP)**2 + np.linalg.norm(hQ)**2)\n",
    "    \n",
    "    return se / total\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.246471264278017"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use a random P and Q to test the function computeMSE\n",
    "# Point 2: report MSE with LAMBDA=0\n",
    "\n",
    "K = 5 # rank parameter\n",
    "P = np.random.rand(M,K) # user's features matrix (M by K)\n",
    "Q = np.random.rand(N,K) # item's features matrix (N by K)\n",
    "\n",
    "computeMSE(train, P, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradient of the objective funtion\n",
    "def computeGradMSE(RDD,P,Q,la=0):\n",
    "    \"\"\" \n",
    "    This function computes the gradient of regularized MSE with respect to P and Q\n",
    "    Args:\n",
    "        RDD: a dict of list of userID, itemID, Rating\n",
    "        P: user's features matrix (M by K)\n",
    "        Q: item's features matrix (N by K)\n",
    "    Returns:\n",
    "        gradP, gradQ: gradient of mse with respect to each element of P and Q \n",
    "    \"\"\" \n",
    "    \n",
    "    gradP = np.zeros(P.shape)\n",
    "    gradQ = np.zeros(Q.shape)\n",
    "    ratings = RDD['ratings']\n",
    "    users = RDD['users']\n",
    "    items = RDD['items']\n",
    "    total = RDD['total']\n",
    "    for n in range(total):\n",
    "        r = ratings[n]\n",
    "        i = dic_users[users[n]]\n",
    "        j = dic_items[items[n]]\n",
    "        hQ = np.copy(Q[j,:])\n",
    "        hQ[hQ > 0] = 0\n",
    "        hP = np.copy(P[i,:])\n",
    "        hP[hP > 0] = 0\n",
    "        e = -2 * (r - Q[j,:].T @ P[i,:]) \n",
    "        for k in range(P.shape[1]):\n",
    "            gradP[i,k] = e * Q[j,k] + 2 * la * hP[k]\n",
    "            gradQ[j,k] = e * P[i,k] + 2 * la * hQ[k]\n",
    "    \n",
    "    return gradP, gradQ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-3.50000517, -1.0024571 , -0.50518826, -0.5781635 , -4.46228871],\n",
       "        [-2.9898363 , -2.07690505, -2.10632066, -2.14615049, -3.08635349],\n",
       "        [-1.52319414, -0.62597446, -1.35041149, -4.31971525, -5.44930643],\n",
       "        ...,\n",
       "        [-4.58705787, -4.34118794, -0.430114  , -1.5375722 , -3.36314647],\n",
       "        [-3.0210971 , -2.45251768, -2.35703026, -3.62604401, -3.15095509],\n",
       "        [-6.22292436, -1.24656442, -1.45889114, -3.61198004, -3.24563299]]),\n",
       " array([[-1.72859797e-01, -4.08245499e-01, -2.91081543e+00,\n",
       "         -3.11748105e+00, -3.12513198e+00],\n",
       "        [-5.08823865e-01, -7.89031606e-01, -7.09682490e-01,\n",
       "         -2.93984392e-02, -8.78913882e-01],\n",
       "        [ 1.42067091e-01,  4.25940891e-02,  8.05929654e-02,\n",
       "          1.58451880e-01,  1.27790238e-01],\n",
       "        ...,\n",
       "        [-3.87601894e-03, -3.59473715e+00, -4.38066937e-01,\n",
       "         -2.72076637e+00, -1.90509537e+00],\n",
       "        [-1.22009845e-01, -2.73943909e+00, -3.87683578e+00,\n",
       "         -2.84336891e+00, -2.02692562e+00],\n",
       "        [-4.48565412e-01, -5.93181390e-02, -4.20266639e-01,\n",
       "         -1.44301580e+00, -4.78631116e-01]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeGradMSE(train, P, Q, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the (steepest) gradient-descent algorithm\n",
    "from tqdm import tqdm\n",
    "\n",
    "def backtraking(RDD, P, Q, gradP, gradQ, beta):\n",
    "    alpha = 1\n",
    "    while computeMSE(RDD, P - alpha * gradP, Q - alpha * gradQ) > computeMSE(RDD, P, Q):\n",
    "        alpha *= beta\n",
    "    return alpha\n",
    "    \n",
    "\n",
    "def GD(RDD,M,N,K,MAXITER=50, GAMMA=0.001, LAMBDA=0.05, adaptive=0):\n",
    "    \"\"\" \n",
    "    This function implemnts the gradient-descent method to minimize the regularized MSE with respect to P and Q\n",
    "    Args:\n",
    "        RDD: a dict of list of users, items, ratings\n",
    "        M: number of users\n",
    "        N: number of items\n",
    "        K: rank parameter\n",
    "        MAXITER: maximal number of iterations (epoches) of GD \n",
    "        GAMMA: step size of GD\n",
    "        LAMBDA: regulization parameter lambda in the mse loss\n",
    "        adaptive: if 0 then use constant step size GD, \n",
    "                  if 1 then use line search to choose the step size automatically\n",
    "    Returns:\n",
    "        P: optimal P found by GD\n",
    "        Q: optimal Q found by GD\n",
    "        lreg_mse: a list of regulized mse values evaluated on RDD, after each iteration\n",
    "        lmse: a list of mse values, evaluated on RDD after each iteration\n",
    "        other scores for analysis purpose\n",
    "    \"\"\"\n",
    "    tol = 0.005\n",
    "    lreg_mse = []\n",
    "    lmse = []\n",
    "    P = np.random.rand(M,K)\n",
    "    Q = np.random.rand(N,K) \n",
    "    alpha = GAMMA\n",
    "    gradP, gradQ = computeGradMSE(RDD, P, Q, LAMBDA)\n",
    "    normP0 = np.linalg.norm(gradP)\n",
    "    normQ0 = np.linalg.norm(gradQ)\n",
    "    for k in tqdm(range(MAXITER)):\n",
    "        saveP = P.copy()\n",
    "        saveQ = Q.copy()\n",
    "        if adaptive == 1:\n",
    "            alpha = backtraking(RDD, P, Q, gradP, gradQ, 0.1)\n",
    "        P -= alpha * gradP\n",
    "        Q -= alpha * gradQ\n",
    "        reg_mse = computeMSE(RDD, P, Q, LAMBDA)\n",
    "        lreg_mse.append(reg_mse)\n",
    "        mse = computeMSE(RDD, P, Q, 0)\n",
    "        lmse.append(computeMSE(RDD, P, Q, 0))\n",
    "        gradP, gradQ = computeGradMSE(RDD, P, Q, LAMBDA)\n",
    "        \n",
    "        vanish = np.linalg.norm(gradP) <= tol * (normP0 + tol) and np.linalg.norm(gradQ) <= tol * (normQ0 + tol)\n",
    "        stag = np.linalg.norm(P - saveP) <= tol * (np.linalg.norm(P) + tol) and np.linalg.norm(Q - saveQ) <= tol * (np.linalg.norm(Q) + tol)\n",
    "\n",
    "        if vanish or stag:\n",
    "            break\n",
    "    \n",
    "    return P, Q, lreg_mse, lmse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 55/200 [05:25<14:14,  5.89s/it]"
     ]
    }
   ],
   "source": [
    "# Compare GD constant step size with GD line search step size\n",
    "# Point 3: Make plots to show how (regularized) MSE changes with respect to GD iterations \n",
    "# Mention your initialization of P,Q, and the stopping criterion of GD\n",
    "\n",
    "P, Q, lreg_mse, lmse = GD(train,M,N,10,MAXITER=200, GAMMA=0.001, LAMBDA=0.3, adaptive=0)\n",
    "\n",
    "plt.plot(lreg_mse)\n",
    "plt.plot(lmse)\n",
    "plt.ylabel('rmse')\n",
    "plt.xlabel('step')\n",
    "plt.title('(reg) mse evolution constant step rate')\n",
    "plt.gca().legend(('lreg_mse','lmse'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [02:13<16:21, 22.30s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "P, Q, lreg_mse, lmse = GD(train,M,N,10,MAXITER=50, GAMMA=0.001, LAMBDA=0.3, adaptive=1)\n",
    "\n",
    "plt.plot(lreg_mse)\n",
    "plt.plot(lmse)\n",
    "plt.ylabel('rmse')\n",
    "plt.xlabel('step')\n",
    "plt.title('(reg) mse evolution line search')\n",
    "plt.gca().legend(('lreg_mse','lmse'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Performance evaluation\n",
    "1. Compute RMSE score. You should use lenskit.metrics.predict.rmse for a fair comparison. Analyze both the training and test score on the 5 cross-validation partitions. \n",
    "2. Compare with a baseline method called Bias. Tune the hyper-parameters such as K and lambda to see if you can obtain a smaller RMSE. Try to explain why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [01:07<12:51, 16.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- train RMSE = 1.266671448536358\n",
      "- test RMSE = 1.351999208774776\n",
      "Sample 1 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [01:41<19:28, 25.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- train RMSE = 1.2375828984647337\n",
      "- test RMSE = 1.3223387419017447\n",
      "Sample 2 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 4/50 [01:07<12:58, 16.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- train RMSE = 1.246443732670737\n",
      "- test RMSE = 1.3278474235491173\n",
      "Sample 3 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [01:52<16:52, 22.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- train RMSE = 1.2520492957744278\n",
      "- test RMSE = 1.35041680255113\n",
      "Sample 4 :\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 5/50 [01:52<16:51, 22.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- train RMSE = 1.2738976533556166\n",
      "- test RMSE = 1.3605951856692993\n",
      "Train rmse mean = 1.2553290057603745\n",
      "Test rmse mean = 1.3426394724892137\n"
     ]
    }
   ],
   "source": [
    "# Point 4: Report RMSE, together with your choice of K,LAMBDA and GD parameters\n",
    "\n",
    "def evaluteRMSE(RDD,P,Q):\n",
    "    \"\"\" \n",
    "    This function computes the root MSE score on the rating of RDD. It compares the rating of each (i,j)\n",
    "    in RDD, with the prediction made by <p_i,q_j>. \n",
    "    Args:\n",
    "        RDD: a dict of list of users, items, ratings\n",
    "        P: optimal P found by GD\n",
    "        Q: optimal Q found by GD\n",
    "    Returns:\n",
    "        RMSE: the RMSE score\n",
    "    \"\"\"\n",
    "    \n",
    "    return np.sqrt(computeMSE(RDD,P,Q))\n",
    "\n",
    "train_rmse_mean = 0\n",
    "test_rmse_mean = 0\n",
    "for i in range(nbSample):\n",
    "    print(f'Sample {i} :')\n",
    "    train = convert_DF_to_RDD(pd.read_csv(csvroot + f'/train-{i}.csv'))\n",
    "    test = convert_DF_to_RDD(pd.read_csv(csvroot + f'/test-{i}.csv'))\n",
    "    P, Q, lreg_mse, lmse = GD(train,M,N,K,MAXITER=50, GAMMA=0.001, LAMBDA=0.05, adaptive=1)\n",
    "    rmse_train = evaluteRMSE(train, P, Q)\n",
    "    train_rmse_mean += rmse_train\n",
    "    print(f'- train RMSE = {rmse_train}')\n",
    "    rmse_test = evaluteRMSE(test, P, Q)\n",
    "    test_rmse_mean += rmse_test\n",
    "    print(f'- test RMSE = {rmse_test}')\n",
    "\n",
    "print(f'Train rmse mean = {train_rmse_mean/nbSample}')\n",
    "print(f'Test rmse mean = {test_rmse_mean/nbSample}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 :\n",
      "- train RMSE = 0.8076072193879845\n",
      "- test RMSE = 0.860220910922801\n",
      "Sample 1 :\n",
      "- train RMSE = 0.8068934004022934\n",
      "- test RMSE = 0.869415425998473\n",
      "Sample 2 :\n",
      "- train RMSE = 0.8097515909752586\n",
      "- test RMSE = 0.8686081588172784\n",
      "Sample 3 :\n",
      "- train RMSE = 0.8049834336808545\n",
      "- test RMSE = 0.8770768812608546\n",
      "Sample 4 :\n",
      "- train RMSE = 0.8062962484538404\n",
      "- test RMSE = 0.8706520992107379\n",
      "Train rmse mean = 0.8071063785800463\n",
      "Test rmse mean = 0.869194695242029\n"
     ]
    }
   ],
   "source": [
    "# Compare the performance with a baseline method called Bias\n",
    "# see in https://lkpy.readthedocs.io/en/stable/bias.html\n",
    "# Point 5:  report the RMSE of the baseline method, and analyze the results\n",
    "# Hint: use read_csv in panda to read you csv data\n",
    "\n",
    "from lenskit.metrics.predict import user_metric, rmse\n",
    "from lenskit.algorithms.bias import Bias\n",
    "from lenskit.batch import predict\n",
    "\n",
    "train_rmse_mean = 0\n",
    "test_rmse_mean = 0\n",
    "for i in range(nbSample):\n",
    "    print(f'Sample {i} :')\n",
    "    train = pd.read_csv(csvroot + f'/train-{i}.csv')\n",
    "    test = pd.read_csv(csvroot + f'/test-{i}.csv')\n",
    "    algo = Bias()\n",
    "    algo.fit(train)\n",
    "    preds_train = predict(algo, train)\n",
    "    rmse_train = user_metric(preds_train, metric=rmse)\n",
    "    train_rmse_mean += rmse_train\n",
    "    print(f'- train RMSE = {rmse_train}')\n",
    "    preds_test = predict(algo, test)\n",
    "    rmse_test = user_metric(preds_test, metric=rmse)\n",
    "    test_rmse_mean += rmse_test\n",
    "    print(f'- test RMSE = {rmse_test}')\n",
    "\n",
    "print(f'Train rmse mean = {train_rmse_mean/nbSample}')\n",
    "print(f'Test rmse mean = {test_rmse_mean/nbSample}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
