{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization for Collabrative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Preprocessing of data\n",
    "\n",
    "1. Data format\n",
    "\n",
    "We are going to preprocess a rating file in the following csv format:  \n",
    "```\n",
    "UserID::MovieID::Rating::Timestamp\n",
    "```\n",
    "\n",
    "2. Prepare data for cross-validation\n",
    "\n",
    "Splitting Data for a user-based N-fold cross-validation. Store each partition into a new csv file. \n",
    "\n",
    "It is convinient to use a python package lenskit. It can be installed by following https://lkpy.readthedocs.io/en/stable/install.html\n",
    "\n",
    "Use the function lenskit.crossfold.partition_rows to partition all the ratings into N train-test partitions.\n",
    "\n",
    "3. Convert to a list format\n",
    "\n",
    "Convert data into a list format for fast processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lenskit\n",
      "  Downloading lenskit-0.14.2-py3-none-any.whl (74 kB)\n",
      "Collecting seedbank>=0.1.0\n",
      "  Downloading seedbank-0.1.2-py3-none-any.whl (7.9 kB)\n",
      "Requirement already satisfied: numba<0.57,>=0.51 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from lenskit) (0.55.1)\n",
      "Collecting csr>=0.3.1\n",
      "  Downloading csr-0.4.3-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: scipy>=1.2 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from lenskit) (1.7.3)\n",
      "Requirement already satisfied: cffi>=1.12.2 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from lenskit) (1.15.0)\n",
      "Requirement already satisfied: psutil>=5 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from lenskit) (5.8.0)\n",
      "Collecting binpickle>=0.3.2\n",
      "  Downloading binpickle-0.3.4-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: pandas==1.*,>=1.0 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from lenskit) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from lenskit) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from pandas==1.*,>=1.0->lenskit) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from pandas==1.*,>=1.0->lenskit) (2021.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from binpickle>=0.3.2->lenskit) (1.0.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from cffi>=1.12.2->lenskit) (2.21)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from numba<0.57,>=0.51->lenskit) (0.38.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from numba<0.57,>=0.51->lenskit) (61.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kraco\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas==1.*,>=1.0->lenskit) (1.16.0)\n",
      "Collecting anyconfig\n",
      "  Downloading anyconfig-0.13.0-py2.py3-none-any.whl (87 kB)\n",
      "Installing collected packages: anyconfig, seedbank, csr, binpickle, lenskit\n",
      "Successfully installed anyconfig-0.13.0 binpickle-0.3.4 csr-0.4.3 lenskit-0.14.2 seedbank-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install lenskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 610 users, 9724 movies and the rating matrix has 1.699968 percent of non-zero value.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read a DataFrame of ratings from the csv ratings file\n",
    "csvroot = 'data'\n",
    "ratings = pd.read_csv(csvroot + '/ratings.csv')\n",
    "ratings = ratings.rename(columns={'userId': 'user', 'movieId': 'item'})\n",
    "\n",
    "def unique(list1):\n",
    "    # insert the list to the set\n",
    "    list_set = set(list1)\n",
    "    # convert the set to the list\n",
    "    unique_list = (list(list_set))\n",
    "    # use a dictionary to store the index of each element in the list\n",
    "    unique_dict = {}\n",
    "    for i in range(len(unique_list)):\n",
    "        unique_dict[unique_list[i]] = i \n",
    "    return unique_list, unique_dict\n",
    "\n",
    "lst_users, dic_users = unique(ratings['user'])\n",
    "lst_items, dic_items = unique(ratings['item'])\n",
    "\n",
    "M = len(lst_users)\n",
    "N = len(lst_items)\n",
    "matrixSparsity = len(ratings) / (M*N)\n",
    "print(\"We have %d users, %d movies and the rating matrix has %f percent of non-zero value.\\n\" % (M, N, 100*matrixSparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 fold cross-validation, store each partition (both train and test) in a seperate csv file.\n",
    "# Point 1: report the number of (user,iter,rating) items in each file.\n",
    "\n",
    "import lenskit.crossfold as xf\n",
    "\n",
    "for i, tp in enumerate(xf.partition_users(ratings, 5, xf.SampleN(5))):\n",
    "    tp.train.to_csv(csvroot + '/train-%d.csv' % (i,))\n",
    "    tp.test.to_csv(csvroot + '/test-%d.csv' % (i,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_DF_to_RDD(DF):\n",
    "    \"\"\" \n",
    "    This function converts a rating dataframe into a dictionary of lists\n",
    "    Args:\n",
    "        DF: a dataframe which contains 'user', 'item' and 'rating' columns\n",
    "    Returns:\n",
    "        RDD: a dictionary which contains \n",
    "            'total': the total number of rating\n",
    "            'users': a list of user id for each rating\n",
    "            'items': a list of item id for each rating\n",
    "            'ratings': a list of ratings\n",
    "    \"\"\" \n",
    "    RDD = {'total':0,'users':[],'items':[],'ratings':[]} \n",
    "    \n",
    "    ### TODO\n",
    "\n",
    "    \n",
    "    return RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the csv files from a partition of the cross-validation\n",
    "# convert them to RDD using convert_DF_to_RDD\n",
    "\n",
    "### TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Gradient-descent algorithm\n",
    "\n",
    "Based on the preprocessing, we are going to develop a method to find optimal P and Q on training data. It contains: \n",
    "\n",
    "1. compute the objective and the gradient of the objective function\n",
    "2. implement the gradient-descent algroithm\n",
    "3. measure the speed of this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume now you have obtained trainRDD and testRDD\n",
    "# Compute the objective funtion\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def computeMSE(RDD,P,Q,la=0):\n",
    "    \"\"\" \n",
    "    This function computes regularized Mean Squared Error (MSE)\n",
    "    Args:\n",
    "        RDD: a dict of list of userID, itemID, Rating\n",
    "        P: user's features matrix (M by K)\n",
    "        Q: item's features matrix (N by K)\n",
    "        la: lambda parameter for regulization (see definition in the project description)\n",
    "    Returns:\n",
    "        mse: mean squared error\n",
    "    \"\"\" \n",
    "    \n",
    "    ### TODO\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a random P and Q to test the function computeMSE\n",
    "# Point 2: report MSE with LAMBDA=0\n",
    "\n",
    "K = 5 # rank parameter\n",
    "P = np.random.rand(M,K) # user's features matrix (M by K)\n",
    "Q = np.random.rand(N,K) # item's features matrix (N by K)\n",
    "\n",
    "### TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the gradient of the objective funtion\n",
    "def computeGradMSE(RDD,P,Q,la=0):\n",
    "    \"\"\" \n",
    "    This function computes the gradient of regularized MSE with respect to P and Q\n",
    "    Args:\n",
    "        RDD: a dict of list of userID, itemID, Rating\n",
    "        P: user's features matrix (M by K)\n",
    "        Q: item's features matrix (N by K)\n",
    "    Returns:\n",
    "        gradP, gradQ: gradient of mse with respect to each element of P and Q \n",
    "    \"\"\" \n",
    "    \n",
    "    ### TODO\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the (steepest) gradient-descent algorithm\n",
    "\n",
    "### TODO\n",
    "\n",
    "\n",
    "def GD(RDD,M,N,K,MAXITER=50, GAMMA=0.001, LAMBDA=0.05, adaptive=0):\n",
    "    \"\"\" \n",
    "    This function implemnts the gradient-descent method to minimize the regularized MSE with respect to P and Q\n",
    "    Args:\n",
    "        RDD: a dict of list of users, items, ratings\n",
    "        M: number of users\n",
    "        N: number of items\n",
    "        K: rank parameter\n",
    "        MAXITER: maximal number of iterations (epoches) of GD \n",
    "        GAMMA: step size of GD\n",
    "        LAMBDA: regulization parameter lambda in the mse loss\n",
    "        adaptive: if 0 then use constant step size GD, \n",
    "                  if 1 then use line search to choose the step size automatically\n",
    "    Returns:\n",
    "        P: optimal P found by GD\n",
    "        Q: optimal Q found by GD\n",
    "        lreg_mse: a list of regulized mse values evaluated on RDD, after each iteration\n",
    "        lmse: a list of mse values, evaluated on RDD after each iteration\n",
    "        other scores for analysis purpose\n",
    "    \"\"\" \n",
    "    \n",
    "    ### TODO\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compare GD constant step size with GD line search step size\n",
    "# Point 3: Make plots to show how (regularized) MSE changes with respect to GD iterations \n",
    "# Mention your initialization of P,Q, and the stopping criterion of GD\n",
    "\n",
    "### TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Perofomance evaluation\n",
    "1. Compute RMSE score. You should use lenskit.metrics.predict.rmse for a fair comparison. Analyze both the training and test score on the 5 cross-validation partitions. \n",
    "2. Compare with a baseline method called Bias. Tune the hyper-parameters such as K and lambda to see if you can obtain a smaller RMSE. Try to explain why. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Point 4: Report RMSE, together with your choice of K,LAMBDA and GD parameters\n",
    "\n",
    "from lenskit.metrics.predict import rmse\n",
    "\n",
    "def evaluteRMSE(RDD,P,Q):\n",
    "    \"\"\" \n",
    "    This function computes the root MSE score on the rating of RDD. It compares the rating of each (i,j)\n",
    "    in RDD, with the prediction made by <p_i,q_j>. \n",
    "    Args:\n",
    "        RDD: a dict of list of users, items, ratings\n",
    "        P: optimal P found by GD\n",
    "        Q: optimal Q found by GD\n",
    "    Returns:\n",
    "        RMSE: the RMSE score\n",
    "    \"\"\"\n",
    "    \n",
    "    ### TODO\n",
    "    \n",
    "\n",
    "### TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the performance with a baseline method called Bias\n",
    "# see in https://lkpy.readthedocs.io/en/stable/bias.html\n",
    "# Point 5:  report the RMSE of the baseline method, and analyze the results\n",
    "# Hint: use read_csv in panda to read you csv data\n",
    "\n",
    "from lenskit.algorithms.bias import Bias\n",
    "from lenskit.batch import predict\n",
    "\n",
    "### TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
